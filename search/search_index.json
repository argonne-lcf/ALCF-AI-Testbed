{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ALCF AI-Testbed Documentation The purpose of this technical documentation is to provide information on using the ALCF AI-Testbed. Please contact ai@alcf.anl.gov for questions or feedback. Cerebras (CS-1) CS-1 is a wafer-scale, deep learning accelerator. Processing, memory, and communication in CS-1 reside in the Cerebras Wafer-Scale Engine (WSE), a 462 square-cm silicon wafer with approximately 400,000 processor cores. Each core has 48 KB of dedicated SRAM memory (for a total of 18GB on-chip), and all cores are connected to one another over a high bandwidth, low latency, two-dimensional interconnect mesh. The software platform integrates popular machine learning frameworks like Tensorflow and PyTorch. SambaNova SambaNova systems aims to develop and accelerate AI applications at scale with a Reconfigurable Dataflow ArchitectureTM (RDA). At the core of this system is a Reconfigurable Dataflow UnitTM (RDU) which is a next-generation processor that provides dataflow processing and acceleration. The software stack, SambaFlowTM, extracts, optimizes and maps dataflow graphs to RDUs from standard machine learning frameworks such as PyTorch and Tensorflow. SambaNova Systems DatascaleTM is a rack-level accelerated system that includes DataScale nodes with integrated networking. The system deployed at ALCF AI testbed is a half-rack RDA system consisting of two nodes, each with eight RDUs. The RDUs on a node are interconnected via a proprietary interconnect to enable both model parallelism as well as data parallelism. Each node consists two sockets with 128 cores and 1 TB of memory and are interconnected using an Infiniband-based fabric. Graphcore Colossus GC2 Intelligent Processing Unit (IPU) was designed to provide state-of-the-art performance for training and inference workloads. It consists of 1216 IPU-Tiles each with an independent core and tightly coupled memory. The Dell DSS8440, the first Graphcore IPUserver, features 8 dual-IPU C2 PCIe cards, all connected with IPU-Link\u2122technology in an industry standard 4U server for AI training and inference workloads. The server has two sockets, each with twenty cores, and 768GB of memory. Groq (Available in 2021) Groq tensor streaming processor (TSP) provides a scalable and programmable processing core and memory building block to achieve 250 TFlops in FP16 and 1 PetaOp/s in INT8 performance. The Groq accelerators are PCIe gen4-based and multiple accelerators one a node can be interconnect via a proprietary chip-to-chip interconnect to enable larger models and data parallelism.","title":"Documentation"},{"location":"#alcf-ai-testbed-documentation","text":"The purpose of this technical documentation is to provide information on using the ALCF AI-Testbed. Please contact ai@alcf.anl.gov for questions or feedback.","title":"ALCF AI-Testbed Documentation"},{"location":"#cerebras-cs-1","text":"CS-1 is a wafer-scale, deep learning accelerator. Processing, memory, and communication in CS-1 reside in the Cerebras Wafer-Scale Engine (WSE), a 462 square-cm silicon wafer with approximately 400,000 processor cores. Each core has 48 KB of dedicated SRAM memory (for a total of 18GB on-chip), and all cores are connected to one another over a high bandwidth, low latency, two-dimensional interconnect mesh. The software platform integrates popular machine learning frameworks like Tensorflow and PyTorch.","title":"Cerebras (CS-1)"},{"location":"#sambanova","text":"SambaNova systems aims to develop and accelerate AI applications at scale with a Reconfigurable Dataflow ArchitectureTM (RDA). At the core of this system is a Reconfigurable Dataflow UnitTM (RDU) which is a next-generation processor that provides dataflow processing and acceleration. The software stack, SambaFlowTM, extracts, optimizes and maps dataflow graphs to RDUs from standard machine learning frameworks such as PyTorch and Tensorflow. SambaNova Systems DatascaleTM is a rack-level accelerated system that includes DataScale nodes with integrated networking. The system deployed at ALCF AI testbed is a half-rack RDA system consisting of two nodes, each with eight RDUs. The RDUs on a node are interconnected via a proprietary interconnect to enable both model parallelism as well as data parallelism. Each node consists two sockets with 128 cores and 1 TB of memory and are interconnected using an Infiniband-based fabric.","title":"SambaNova"},{"location":"#graphcore","text":"Colossus GC2 Intelligent Processing Unit (IPU) was designed to provide state-of-the-art performance for training and inference workloads. It consists of 1216 IPU-Tiles each with an independent core and tightly coupled memory. The Dell DSS8440, the first Graphcore IPUserver, features 8 dual-IPU C2 PCIe cards, all connected with IPU-Link\u2122technology in an industry standard 4U server for AI training and inference workloads. The server has two sockets, each with twenty cores, and 768GB of memory.","title":"Graphcore"},{"location":"#groq-available-in-2021","text":"Groq tensor streaming processor (TSP) provides a scalable and programmable processing core and memory building block to achieve 250 TFlops in FP16 and 1 PetaOp/s in INT8 performance. The Groq accelerators are PCIe gen4-based and multiple accelerators one a node can be interconnect via a proprietary chip-to-chip interconnect to enable larger models and data parallelism.","title":"Groq (Available in 2021)"},{"location":"cerebras/","text":"CS-1 Quickstart Guide for ALCF November 2020 Note: For support or questions, please contact support@cerebras.net . This guide provides a quickstart guide to running a training job on the ALCF CS-1 cluster with SLURM, using a simple FC model as a demo model. For more details on how to prepare a model for CS-1 and on the execution model, please see the full CS-1 ML User Guide. 1. Log in to the CS-1 medulla cluster at ALCF In order to login, you need a CELS ldap account. Go to https://accounts.cels.anl.gov/ and create an account. After that, request to join a project named \"Cerebras\". The CS-1 support cluster accepts connections from CELS homes nodes (homes.cels.anl.gov). You can connect to CELS homes nodes first and ssh to cs1. See https://virtualhelpdesk.cels.anl.gov/docs/linux/login-compute-and-home-nodes/ for connecting homes nodes. For your convenience, you can set an ssh proxy as below, # ssh proxy setting ~/.ssh/config ControlPath ~/.ssh/.control_channels/%h:%p:%r Host login-gce User [username] Hostname logins.cels.anl.gov ControlMaster auto ControlPersist yes LogLevel FATAL Host homes-gce User [username] Hostname homes.cels.anl.gov ProxyCommand ssh login-gce -q -W %h:%p ForwardX11 yes Host cs1-gce HostName cs1.cels.anl.gov ProxyCommand ssh login-gce -q -W %h:%p and ssh cs1-gce 2. Clone model zoo repository to home directory Currently, the Cerebras ModelZoo repo is private. Please contact support@cerebras.net , cc: jessica@cerebras.net with your Github username and \"UChicago ModelZoo Access\" in the subject line. git clone git@github.com:Cerebras/modelzoo.git In the modelzoo directory, you should see several examples. To name a few: fc_mnist , A simple multi-layer perceptron model composed of fully-connected layers for performing handwriting recognition on the MNIST dataset. Check README.md for more detail. bert , BERT model implementation. Check README.md for more detail. rnn_sentiment , LSTM-based sentiment analysis model that predicts sentiment type on a passage Model zoo walkthrough video is available here. For the purposes of this quickstart, we will use the fc_mnist model. cd modelzoo/fc_mnist/tf/ 3. Running a model with Slurm on CS-1 For more details on Slurm options please see the full CS-1 User Guide section Executing a training job on CS-1 with Slurm . The common below trains the simple FC model for 100,000 steps, takes a checkpoint every 10,000 steps, and tells Slurm to launch 1 input worker node to feed the training job. The num_steps and save_checkpoints_steps parameters can also be set in the params.yml file. NUM_WORKER_NODES=1 srun_train python run.py --mode train --cs_ip 10.80.0.100 --max_steps 100000 After running this command, you should see similar output to the below: srun: job 5834 queued and waiting for resources srun: job 5834 has been allocated resources ... INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Saving checkpoints for 0 into model_dir/model.ckpt. INFO:tensorflow:Programming CS-1 fabric. This may take a couple of minutes - please do not interrupt. INFO:tensorflow:Fabric programmed INFO:tensorflow:Coordinator fully up. Waiting for Streaming (using 0.97% out of 301600 cores on the fabric) INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. ... INFO:tensorflow:Training finished with 25600000 samples in 187.465 seconds, 136558.69 samples / second INFO:tensorflow:Saving checkpoints for 100000 into model_dir/model.ckpt. INFO:tensorflow:global step 100000: loss = 1.901388168334961e-05 (532.0 steps/sec) INFO:tensorflow:global step 100000: loss = 1.901388168334961e-05 (532.0 steps/sec) INFO:tensorflow:Loss for final step: 1.9e-05. Training and Evaluating on CPU To train and eval on CPU, run the Cerebras Singularity container in interactive mode, then run the run.py script, specifying \u2018train\u2019 or \u2018eval\u2019 for the \u2018--mode\u2019 script argument. In these scripts, if the cs_ip address is not specified, it will automatically run on CPU. 1. Navigate to model directory cd modelzoo/fc_mnist/tf/ 2. Run salloc_node, which will both start the Cerebras Singularity container in interactive shell mode and reserve a worker CPU node in the CS-1 Slurm cluster salloc_node 3. Train and evaluate the model on CPU # train the simple model on CPU Singularity> python run.py --mode train # run eval on the simple model on CPU Singularity> python run.py --mode eval Fast iteration and precompilation on CPU CerebrasEstimator provides a lightweight model \"verification\" mode through the compile() function. In this mode, compile() will only run through the first few stages of the Cerebras Graph Compiler (CGC), up until kernel library matching. This step is very fast and allows you to quickly iterate on your model code and determine if you are using any TensorFlow layers or functionality that is unsupported by either XLA or CGC. 1. Navigate to model directory cd modelzoo/fc_mnist/tf/ 2. Run the Cerebras Singularity container in interactive shell mode and reserve a worker CPU node in the Cerebras cluster # do not use salloc_node singularity shell -B /data /data/shared/software/singularity/cbcore_latest.sif 3. Run the compilation process in validate_only mode Singularity> python run.py --mode validate_only ... XLA Extraction Complete =============== Starting Cerebras Compilation =============== Cerebras compilation completed: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02s, 1.23s/stages] =============== Cerebras Compilation Completed =============== validate_only mode checks kernel compatibility of your model. Once your model passes, run the full compilation to generate the CS-1 executable. 4. Run the full compilation process in compile_only mode This runs the full compilation through all stages of the Cerebras software stack to generate a CS-1 executable. Singularity> python run.py --mode compile_only ... XLA Extraction Complete =============== Starting Cerebras Compilation =============== Cerebras compilation completed: | | 17/? [00:18s, 1.09s/stages] =============== Cerebras Compilation Completed =============== If this is successful, your model is guaranteed to run on CS-1. You can also use this mode to run pre-compilations of many different model configurations offline, so you can more fully utilize allotted CS-1 cluster time. CGC will detect if a binary already exists for a particular model config and will skip compiling on-the-fly during training if it detects one.","title":"Cerebras (CS-1)"},{"location":"cerebras/#cs-1-quickstart-guide-for-alcf","text":"November 2020 Note: For support or questions, please contact support@cerebras.net . This guide provides a quickstart guide to running a training job on the ALCF CS-1 cluster with SLURM, using a simple FC model as a demo model. For more details on how to prepare a model for CS-1 and on the execution model, please see the full CS-1 ML User Guide.","title":"CS-1 Quickstart Guide for ALCF"},{"location":"cerebras/#1-log-in-to-the-cs-1-medulla-cluster-at-alcf","text":"In order to login, you need a CELS ldap account. Go to https://accounts.cels.anl.gov/ and create an account. After that, request to join a project named \"Cerebras\". The CS-1 support cluster accepts connections from CELS homes nodes (homes.cels.anl.gov). You can connect to CELS homes nodes first and ssh to cs1. See https://virtualhelpdesk.cels.anl.gov/docs/linux/login-compute-and-home-nodes/ for connecting homes nodes. For your convenience, you can set an ssh proxy as below, # ssh proxy setting ~/.ssh/config ControlPath ~/.ssh/.control_channels/%h:%p:%r Host login-gce User [username] Hostname logins.cels.anl.gov ControlMaster auto ControlPersist yes LogLevel FATAL Host homes-gce User [username] Hostname homes.cels.anl.gov ProxyCommand ssh login-gce -q -W %h:%p ForwardX11 yes Host cs1-gce HostName cs1.cels.anl.gov ProxyCommand ssh login-gce -q -W %h:%p and ssh cs1-gce","title":"1. Log in to the CS-1 medulla cluster at ALCF"},{"location":"cerebras/#2-clone-model-zoo-repository-to-home-directory","text":"Currently, the Cerebras ModelZoo repo is private. Please contact support@cerebras.net , cc: jessica@cerebras.net with your Github username and \"UChicago ModelZoo Access\" in the subject line. git clone git@github.com:Cerebras/modelzoo.git In the modelzoo directory, you should see several examples. To name a few: fc_mnist , A simple multi-layer perceptron model composed of fully-connected layers for performing handwriting recognition on the MNIST dataset. Check README.md for more detail. bert , BERT model implementation. Check README.md for more detail. rnn_sentiment , LSTM-based sentiment analysis model that predicts sentiment type on a passage Model zoo walkthrough video is available here. For the purposes of this quickstart, we will use the fc_mnist model. cd modelzoo/fc_mnist/tf/","title":"2. Clone model zoo repository to home directory"},{"location":"cerebras/#3-running-a-model-with-slurm-on-cs-1","text":"For more details on Slurm options please see the full CS-1 User Guide section Executing a training job on CS-1 with Slurm . The common below trains the simple FC model for 100,000 steps, takes a checkpoint every 10,000 steps, and tells Slurm to launch 1 input worker node to feed the training job. The num_steps and save_checkpoints_steps parameters can also be set in the params.yml file. NUM_WORKER_NODES=1 srun_train python run.py --mode train --cs_ip 10.80.0.100 --max_steps 100000 After running this command, you should see similar output to the below: srun: job 5834 queued and waiting for resources srun: job 5834 has been allocated resources ... INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Saving checkpoints for 0 into model_dir/model.ckpt. INFO:tensorflow:Programming CS-1 fabric. This may take a couple of minutes - please do not interrupt. INFO:tensorflow:Fabric programmed INFO:tensorflow:Coordinator fully up. Waiting for Streaming (using 0.97% out of 301600 cores on the fabric) INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. ... INFO:tensorflow:Training finished with 25600000 samples in 187.465 seconds, 136558.69 samples / second INFO:tensorflow:Saving checkpoints for 100000 into model_dir/model.ckpt. INFO:tensorflow:global step 100000: loss = 1.901388168334961e-05 (532.0 steps/sec) INFO:tensorflow:global step 100000: loss = 1.901388168334961e-05 (532.0 steps/sec) INFO:tensorflow:Loss for final step: 1.9e-05.","title":"3. Running a model with Slurm on CS-1"},{"location":"cerebras/#training-and-evaluating-on-cpu","text":"To train and eval on CPU, run the Cerebras Singularity container in interactive mode, then run the run.py script, specifying \u2018train\u2019 or \u2018eval\u2019 for the \u2018--mode\u2019 script argument. In these scripts, if the cs_ip address is not specified, it will automatically run on CPU.","title":"Training and Evaluating on CPU"},{"location":"cerebras/#1-navigate-to-model-directory","text":"cd modelzoo/fc_mnist/tf/","title":"1. Navigate to model directory"},{"location":"cerebras/#2-run-salloc_node-which-will-both-start-the-cerebras-singularity-container-in-interactive-shell-mode-and-reserve-a-worker-cpu-node-in-the-cs-1-slurm-cluster","text":"salloc_node","title":"2. Run salloc_node, which will both start the Cerebras Singularity container in interactive shell mode and reserve a worker CPU node in the CS-1 Slurm cluster"},{"location":"cerebras/#3-train-and-evaluate-the-model-on-cpu","text":"# train the simple model on CPU Singularity> python run.py --mode train # run eval on the simple model on CPU Singularity> python run.py --mode eval","title":"3. Train and evaluate the model on CPU"},{"location":"cerebras/#fast-iteration-and-precompilation-on-cpu","text":"CerebrasEstimator provides a lightweight model \"verification\" mode through the compile() function. In this mode, compile() will only run through the first few stages of the Cerebras Graph Compiler (CGC), up until kernel library matching. This step is very fast and allows you to quickly iterate on your model code and determine if you are using any TensorFlow layers or functionality that is unsupported by either XLA or CGC.","title":"Fast iteration and precompilation on CPU"},{"location":"cerebras/#1-navigate-to-model-directory_1","text":"cd modelzoo/fc_mnist/tf/","title":"1. Navigate to model directory"},{"location":"cerebras/#2-run-the-cerebras-singularity-container-in-interactive-shell-mode-and-reserve-a-worker-cpu-node-in-the-cerebras-cluster","text":"# do not use salloc_node singularity shell -B /data /data/shared/software/singularity/cbcore_latest.sif","title":"2. Run the Cerebras Singularity container in interactive shell mode and reserve a worker CPU node in the Cerebras cluster"},{"location":"cerebras/#3-run-the-compilation-process-in-validate_only-mode","text":"Singularity> python run.py --mode validate_only ... XLA Extraction Complete =============== Starting Cerebras Compilation =============== Cerebras compilation completed: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02s, 1.23s/stages] =============== Cerebras Compilation Completed =============== validate_only mode checks kernel compatibility of your model. Once your model passes, run the full compilation to generate the CS-1 executable.","title":"3. Run the compilation process in validate_only mode"},{"location":"cerebras/#4-run-the-full-compilation-process-in-compile_only-mode","text":"This runs the full compilation through all stages of the Cerebras software stack to generate a CS-1 executable. Singularity> python run.py --mode compile_only ... XLA Extraction Complete =============== Starting Cerebras Compilation =============== Cerebras compilation completed: | | 17/? [00:18s, 1.09s/stages] =============== Cerebras Compilation Completed =============== If this is successful, your model is guaranteed to run on CS-1. You can also use this mode to run pre-compilations of many different model configurations offline, so you can more fully utilize allotted CS-1 cluster time. CGC will detect if a binary already exists for a particular model config and will skip compiling on-the-fly during training if it detects one.","title":"4. Run the full compilation process in compile_only mode"},{"location":"sambanova/","text":"Getting Started with SambaNova Accounts: 1. Get an account on the SambaNova (SN) system. Contact Venkat Vishwanath ( venkat@anl.gov ) for access. 2. You will first need to login to one of the MCS or CELS machines first and then ssh to the SambaNova system that has 2 nodes sm-01 and sm-02 . Hostname for SambaNova is sm-01.cels.anl.gov and sm-02.cels.anl.gov Documents: SambaNova technical documents to read through. Part 1 is H/W and Part 2 is S/W These are under NDA. Please don't share. If you are unable to access these documents, please reach out to Venkat/Murali for access. There is a dedicated documentation portal, please reach out to Murali Emani ( memani@anl.gov ) for access. First Steps: After logging in, you need to activate SN virtual environment (venv) before running models. Activating venv environment: source /opt/sambaflow/venv/bin/activate Deactivating venv environment: deactivate How to Run To run an application on the SambaNova nodes, it has to be written in 'SambaFlow' which is similar to PyTorch. This software stack includes the compilers, runtime and SambaFlow Python SDK. It is to be noted not all operators are supported yet, they will be released in monthly releases. Support for Tensorflow is work in progress. The workflow includes the following four steps to run a model. A high-level overview is mentioned here. Detailed information can be obtained from the official documentation. 1. Compile: Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware like many compute and memory resources are required, and will be used in all subsequent steps. The pef files are usually saved in 'out' directory, it is advised to save it in a separate dir with '--output-folder' option python myapp.py compile --pef-name=\"myapp.pef\" 2. Test (optional): Runs a test both on the host CPU and SN node and will raise errors if any discrepancies are found. Pass the pef file generated above as the input. python myapp.py test --pef=\"out/myapp/myapp.pef\" 3. Run: This will run the application on SN nodes. python myapp.py run --pef=\"out/myapp/myapp.pef\" 4. Measure Performance: This step will report the measured performance. The parameters depend on the model and can include latency, samples/sec. python myapp.py measure-performance --pef=\"out/myapp/myapp.pef Other parameters can be found with --help command in any step, (ex. python myapp,py compile --help ) There are some sample programs at /opt/sambaflow/apps/ to try out. These steps can be submitted in a single script via Slurm using sbatch command. sbatch --output=<path>/output.log submit-job.sh The node and number of resources required can be passed as arguments to sbatch sbatch --output=<path>/output.log -w <node: sm-01/sm-2> --cpus-per-task=4 --gres=rdu:1 submit-job.sh For any other questions, please reach out to Venkat ( venkat@anl.gov ), Murali ( memani@anl.gov ).","title":"SambaNova"},{"location":"sambanova/#getting-started-with-sambanova","text":"","title":"Getting Started with SambaNova"},{"location":"sambanova/#accounts","text":"","title":"Accounts:"},{"location":"sambanova/#1-get-an-account-on-the-sambanova-sn-system-contact-venkat-vishwanath-amp118amp101amp110amp107amp97amp116amp64amp97amp110amp108amp46amp103amp111amp118-for-access","text":"","title":"1. Get an account on the SambaNova (SN) system. Contact Venkat Vishwanath (\u0002amp\u0003#118;\u0002amp\u0003#101;\u0002amp\u0003#110;\u0002amp\u0003#107;\u0002amp\u0003#97;\u0002amp\u0003#116;\u0002amp\u0003#64;\u0002amp\u0003#97;\u0002amp\u0003#110;\u0002amp\u0003#108;\u0002amp\u0003#46;\u0002amp\u0003#103;\u0002amp\u0003#111;\u0002amp\u0003#118;) for access."},{"location":"sambanova/#2-you-will-first-need-to-login-to-one-of-the-mcs-or-cels-machines-first-and-then-ssh-to-the-sambanova-system-that-has-2-nodes-sm-01-and-sm-02-hostname-for-sambanova-is-sm-01celsanlgov-and-sm-02celsanlgov","text":"","title":"2. You will first need to login to one of the MCS or CELS machines first and then ssh to the SambaNova system that has 2 nodes sm-01 and sm-02. Hostname for SambaNova is sm-01.cels.anl.gov and sm-02.cels.anl.gov"},{"location":"sambanova/#documents","text":"SambaNova technical documents to read through. Part 1 is H/W and Part 2 is S/W These are under NDA. Please don't share. If you are unable to access these documents, please reach out to Venkat/Murali for access. There is a dedicated documentation portal, please reach out to Murali Emani ( memani@anl.gov ) for access.","title":"Documents:"},{"location":"sambanova/#first-steps","text":"After logging in, you need to activate SN virtual environment (venv) before running models. Activating venv environment: source /opt/sambaflow/venv/bin/activate Deactivating venv environment: deactivate","title":"First Steps:"},{"location":"sambanova/#how-to-run","text":"To run an application on the SambaNova nodes, it has to be written in 'SambaFlow' which is similar to PyTorch. This software stack includes the compilers, runtime and SambaFlow Python SDK. It is to be noted not all operators are supported yet, they will be released in monthly releases. Support for Tensorflow is work in progress. The workflow includes the following four steps to run a model. A high-level overview is mentioned here. Detailed information can be obtained from the official documentation.","title":"How to Run"},{"location":"sambanova/#1-compile-compiles-the-model-and-generates-a-pef-file-this-file-contains-information-on-how-to-reconfigure-the-hardware-like-many-compute-and-memory-resources-are-required-and-will-be-used-in-all-subsequent-steps-the-pef-files-are-usually-saved-in-out-directory-it-is-advised-to-save-it-in-a-separate-dir-with-output-folder-option","text":"python myapp.py compile --pef-name=\"myapp.pef\"","title":"1. Compile: Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware like many compute and memory resources are required, and will be used in all subsequent steps. The pef files are usually saved in 'out' directory, it is advised to save it in a separate dir with '--output-folder' option"},{"location":"sambanova/#2-test-optional-runs-a-test-both-on-the-host-cpu-and-sn-node-and-will-raise-errors-if-any-discrepancies-are-found-pass-the-pef-file-generated-above-as-the-input","text":"python myapp.py test --pef=\"out/myapp/myapp.pef\"","title":"2. Test (optional): Runs a test both on the host CPU and SN node and will raise errors if any discrepancies are found. Pass the pef file generated above as the input."},{"location":"sambanova/#3-run-this-will-run-the-application-on-sn-nodes","text":"python myapp.py run --pef=\"out/myapp/myapp.pef\"","title":"3. Run: This will run the application on SN nodes."},{"location":"sambanova/#4-measure-performance-this-step-will-report-the-measured-performance-the-parameters-depend-on-the-model-and-can-include-latency-samplessec","text":"python myapp.py measure-performance --pef=\"out/myapp/myapp.pef Other parameters can be found with --help command in any step, (ex. python myapp,py compile --help ) There are some sample programs at /opt/sambaflow/apps/ to try out. These steps can be submitted in a single script via Slurm using sbatch command. sbatch --output=<path>/output.log submit-job.sh The node and number of resources required can be passed as arguments to sbatch sbatch --output=<path>/output.log -w <node: sm-01/sm-2> --cpus-per-task=4 --gres=rdu:1 submit-job.sh For any other questions, please reach out to Venkat ( venkat@anl.gov ), Murali ( memani@anl.gov ).","title":"4. Measure Performance: This step will report the measured performance. The parameters depend on the model and can include latency, samples/sec."}]}